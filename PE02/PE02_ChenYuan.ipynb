{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54f7caa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "######## Exercise 1: Enhanced Text Preprocessing ########\n",
    "#########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57970ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Output Comparison:\n",
      "                                         review_text  \\\n",
      "0  This movie was absolutely fantastic, truly a m...   \n",
      "1    It was okay, not great, not terrible, just meh.   \n",
      "2  Terrible acting and a boring plot. A complete ...   \n",
      "3  A surprisingly good film with compelling chara...   \n",
      "4  I hated every minute. The worst movie I've see...   \n",
      "\n",
      "                                         nltk_tokens  \\\n",
      "0  [movie, absolutely, fantastic, truly, masterpi...   \n",
      "1                       [okay, great, terrible, meh]   \n",
      "2  [terrible, acting, boring, plot, complete, was...   \n",
      "3  [surprisingly, good, film, compelling, charact...   \n",
      "4   [hated, every, minute, worst, movie, seen, year]   \n",
      "\n",
      "                                        spacy_tokens  \n",
      "0  [movie, absolutely, fantastic, truly, masterpi...  \n",
      "1                       [okay, great, terrible, meh]  \n",
      "2  [terrible, acting, boring, plot, complete, was...  \n",
      "3  [surprisingly, good, film, compelling, charact...  \n",
      "4          [hated, minute, worst, movie, seen, year]  \n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------\n",
    "# Task 1: Compare NLTK v.s. spaCy preprocessing\n",
    "#--------------------------------------------------------\n",
    "import nltk\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Create small dataset\n",
    "data = {\n",
    "    'review_text': [\n",
    "        \"This movie was absolutely fantastic, truly a masterpiece!\",\n",
    "        \"It was okay, not great, not terrible, just meh.\",\n",
    "        \"Terrible acting and a boring plot. A complete waste of time.\",\n",
    "        \"A surprisingly good film with compelling characters.\",\n",
    "        \"I hated every minute. The worst movie I've seen all year.\"\n",
    "    ],\n",
    "    'sentiment': [1, 0, 0, 1, 0]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# NLTK preprocessing function\n",
    "def nltk_preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())  # Tokenize and lowercase\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered = [w for w in tokens if w.isalnum() and w not in stop_words]\n",
    "    return filtered\n",
    "\n",
    "# spaCy preprocessing function\n",
    "def spacy_preprocess(text):\n",
    "    doc = nlp(text)\n",
    "    filtered = [token.lower_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "    return filtered\n",
    "\n",
    "# Apply both functions\n",
    "df['nltk_tokens'] = df['review_text'].apply(nltk_preprocess)\n",
    "df['spacy_tokens'] = df['review_text'].apply(spacy_preprocess)\n",
    "\n",
    "print(\"Preprocessing Output Comparison:\")\n",
    "print(df[['review_text', 'nltk_tokens', 'spacy_tokens']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e84c1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step-by-step NLTK preprocessing analysis:\n",
      "\n",
      "Original: This movie was absolutely fantastic, truly a masterpiece!\n",
      "NLTK Tokens: ['This', 'movie', 'was', 'absolutely', 'fantastic', ',', 'truly', 'a', 'masterpiece', '!']\n",
      "Lowercase: ['this', 'movie', 'was', 'absolutely', 'fantastic', ',', 'truly', 'a', 'masterpiece', '!']\n",
      "Filtered: ['movie', 'absolutely', 'fantastic', 'truly', 'masterpiece']\n",
      "Token Count: 5\n",
      "----------------------------------------\n",
      "Original: It was okay, not great, not terrible, just meh.\n",
      "NLTK Tokens: ['It', 'was', 'okay', ',', 'not', 'great', ',', 'not', 'terrible', ',', 'just', 'meh', '.']\n",
      "Lowercase: ['it', 'was', 'okay', ',', 'not', 'great', ',', 'not', 'terrible', ',', 'just', 'meh', '.']\n",
      "Filtered: ['okay', 'great', 'terrible', 'meh']\n",
      "Token Count: 4\n",
      "----------------------------------------\n",
      "Original: Terrible acting and a boring plot. A complete waste of time.\n",
      "NLTK Tokens: ['Terrible', 'acting', 'and', 'a', 'boring', 'plot', '.', 'A', 'complete', 'waste', 'of', 'time', '.']\n",
      "Lowercase: ['terrible', 'acting', 'and', 'a', 'boring', 'plot', '.', 'a', 'complete', 'waste', 'of', 'time', '.']\n",
      "Filtered: ['terrible', 'acting', 'boring', 'plot', 'complete', 'waste', 'time']\n",
      "Token Count: 7\n",
      "----------------------------------------\n",
      "Original: A surprisingly good film with compelling characters.\n",
      "NLTK Tokens: ['A', 'surprisingly', 'good', 'film', 'with', 'compelling', 'characters', '.']\n",
      "Lowercase: ['a', 'surprisingly', 'good', 'film', 'with', 'compelling', 'characters', '.']\n",
      "Filtered: ['surprisingly', 'good', 'film', 'compelling', 'characters']\n",
      "Token Count: 5\n",
      "----------------------------------------\n",
      "Original: I hated every minute. The worst movie I've seen all year.\n",
      "NLTK Tokens: ['I', 'hated', 'every', 'minute', '.', 'The', 'worst', 'movie', 'I', \"'ve\", 'seen', 'all', 'year', '.']\n",
      "Lowercase: ['i', 'hated', 'every', 'minute', '.', 'the', 'worst', 'movie', 'i', \"'ve\", 'seen', 'all', 'year', '.']\n",
      "Filtered: ['hated', 'every', 'minute', 'worst', 'movie', 'seen', 'year']\n",
      "Token Count: 7\n",
      "----------------------------------------\n",
      "\n",
      "Token Counts:\n",
      "                                         review_text  nltk_token_count  \\\n",
      "0  This movie was absolutely fantastic, truly a m...                 5   \n",
      "1    It was okay, not great, not terrible, just meh.                 4   \n",
      "2  Terrible acting and a boring plot. A complete ...                 7   \n",
      "3  A surprisingly good film with compelling chara...                 5   \n",
      "4  I hated every minute. The worst movie I've see...                 7   \n",
      "\n",
      "   spacy_token_count  \n",
      "0                  5  \n",
      "1                  4  \n",
      "2                  7  \n",
      "3                  5  \n",
      "4                  6  \n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------\n",
    "# Task 2: Analyze preprocessing steps \n",
    "#--------------------------------------------------------\n",
    "def analyze_steps(text):\n",
    "    print(f\"Original: {text}\")\n",
    "    # Basic tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    print(f\"NLTK Tokens: {tokens}\")\n",
    "    # Case normalization\n",
    "    tokens_lower = [t.lower() for t in tokens]\n",
    "    print(f\"Lowercase: {tokens_lower}\")\n",
    "    # Stopword removal + punctuation filtering\n",
    "    filtered = [t for t in tokens_lower if t.isalnum() and t not in stopwords.words('english')]\n",
    "    print(f\"Filtered: {filtered}\")\n",
    "    print(f\"Token Count: {len(filtered)}\")\n",
    "    print('-' * 40)\n",
    "\n",
    "print(\"\\nStep-by-step NLTK preprocessing analysis:\\n\")\n",
    "for review in df['review_text']:\n",
    "    analyze_steps(review)\n",
    "\n",
    "# Token count comparison\n",
    "df['nltk_token_count'] = df['nltk_tokens'].apply(len)\n",
    "df['spacy_token_count'] = df['spacy_tokens'].apply(len)\n",
    "\n",
    "print(\"\\nToken Counts:\")\n",
    "print(df[['review_text', 'nltk_token_count', 'spacy_token_count']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f1d7339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing differences between NLTK and spaCy preprocessing:\n",
      "\n",
      "Review 1: This movie was absolutely fantastic, truly a masterpiece!\n",
      "Tokens only in NLTK: set()\n",
      "Tokens only in spaCy: set()\n",
      "Common tokens: {'movie', 'fantastic', 'masterpiece', 'absolutely', 'truly'}\n",
      "\n",
      "Review 2: It was okay, not great, not terrible, just meh.\n",
      "Tokens only in NLTK: set()\n",
      "Tokens only in spaCy: set()\n",
      "Common tokens: {'okay', 'meh', 'terrible', 'great'}\n",
      "\n",
      "Review 3: Terrible acting and a boring plot. A complete waste of time.\n",
      "Tokens only in NLTK: set()\n",
      "Tokens only in spaCy: set()\n",
      "Common tokens: {'plot', 'waste', 'boring', 'acting', 'terrible', 'time', 'complete'}\n",
      "\n",
      "Review 4: A surprisingly good film with compelling characters.\n",
      "Tokens only in NLTK: set()\n",
      "Tokens only in spaCy: set()\n",
      "Common tokens: {'good', 'film', 'characters', 'compelling', 'surprisingly'}\n",
      "\n",
      "Review 5: I hated every minute. The worst movie I've seen all year.\n",
      "Tokens only in NLTK: {'every'}\n",
      "Tokens only in spaCy: set()\n",
      "Common tokens: {'hated', 'year', 'seen', 'movie', 'minute', 'worst'}\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------\n",
    "# Task 3: Document and Analyze Differences\n",
    "#--------------------------------------------------------\n",
    "\n",
    "print(\"\\nAnalyzing differences between NLTK and spaCy preprocessing:\")\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    nltk_set = set(row['nltk_tokens'])\n",
    "    spacy_set = set(row['spacy_tokens'])\n",
    "    \n",
    "    print(f\"\\nReview {i+1}: {row['review_text']}\")\n",
    "    print(f\"Tokens only in NLTK: {nltk_set - spacy_set}\")\n",
    "    print(f\"Tokens only in spaCy: {spacy_set - nltk_set}\")\n",
    "    print(f\"Common tokens: {nltk_set & spacy_set}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b833c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "######### Exercise 2: Basic Feature Extraction ##########\n",
    "#########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25a6b852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 CountVectorizer Features:\n",
      "['absolutely' 'acting' 'boring' 'characters' 'compelling' 'complete'\n",
      " 'every' 'fantastic' 'film' 'good' 'great' 'hated' 'masterpiece' 'meh'\n",
      " 'minute' 'movie' 'okay' 'plot' 'seen' 'surprisingly' 'terrible' 'time'\n",
      " 'truly' 'waste' 'worst' 'year']\n",
      "\n",
      "🔹 TF-IDF Features:\n",
      "['absolutely' 'acting' 'boring' 'characters' 'compelling' 'complete'\n",
      " 'every' 'fantastic' 'film' 'good' 'great' 'hated' 'masterpiece' 'meh'\n",
      " 'minute' 'movie' 'okay' 'plot' 'seen' 'surprisingly' 'terrible' 'time'\n",
      " 'truly' 'waste' 'worst' 'year']\n",
      "\n",
      "CountVectorizer shape: (5, 26)\n",
      "TF-IDF shape: (5, 26)\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------\n",
    "# Task 1: Compare CountVectorizer and TdidfVectorizer \n",
    "#--------------------------------------------------------\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Use the preprocessed NLTK text for fair comparison\n",
    "df['text_clean'] = df['nltk_tokens'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "# Initialize vectorizers\n",
    "count_vec = CountVectorizer()\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the text\n",
    "X_count = count_vec.fit_transform(df['text_clean'])\n",
    "X_tfidf = tfidf_vec.fit_transform(df['text_clean'])\n",
    "\n",
    "# Extract features\n",
    "count_features = count_vec.get_feature_names_out()\n",
    "tfidf_features = tfidf_vec.get_feature_names_out()\n",
    "\n",
    "print(\"🔹 CountVectorizer Features:\")\n",
    "print(count_features)\n",
    "\n",
    "print(\"\\n🔹 TF-IDF Features:\")\n",
    "print(tfidf_features)\n",
    "\n",
    "print(f\"\\nCountVectorizer shape: {X_count.shape}\")\n",
    "print(f\"TF-IDF shape: {X_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d5a4af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔸 N-gram range: (1, 1)\n",
      "Feature count: 26\n",
      "Sample features: ['absolutely' 'acting' 'boring' 'characters' 'compelling' 'complete'\n",
      " 'every' 'fantastic' 'film' 'good']\n",
      "\n",
      "🔸 N-gram range: (1, 2)\n",
      "Feature count: 49\n",
      "Sample features: ['absolutely' 'absolutely fantastic' 'acting' 'acting boring' 'boring'\n",
      " 'boring plot' 'characters' 'compelling' 'compelling characters'\n",
      " 'complete']\n",
      "\n",
      "🔸 N-gram range: (2, 2)\n",
      "Feature count: 23\n",
      "Sample features: ['absolutely fantastic' 'acting boring' 'boring plot'\n",
      " 'compelling characters' 'complete waste' 'every minute' 'fantastic truly'\n",
      " 'film compelling' 'good film' 'great terrible']\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------\n",
    "# Task 2: Compare CountVectorizer and TdidfVectorizer \n",
    "#--------------------------------------------------------\n",
    "\n",
    "# Try different n-gram ranges\n",
    "ngram_configs = [(1,1), (1,2), (2,2)]\n",
    "\n",
    "for ngram_range in ngram_configs:\n",
    "    print(f\"\\n🔸 N-gram range: {ngram_range}\")\n",
    "    \n",
    "    vec = CountVectorizer(ngram_range=ngram_range)\n",
    "    X_ng = vec.fit_transform(df['text_clean'])\n",
    "    features = vec.get_feature_names_out()\n",
    "    \n",
    "    print(f\"Feature count: {len(features)}\")\n",
    "    print(f\"Sample features: {features[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae18225b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Review 1:\n",
      "  truly: 0.4637\n",
      "  masterpiece: 0.4637\n",
      "  fantastic: 0.4637\n",
      "  absolutely: 0.4637\n",
      "  movie: 0.3741\n",
      "\n",
      "📝 Review 2:\n",
      "  okay: 0.5234\n",
      "  meh: 0.5234\n",
      "  great: 0.5234\n",
      "  terrible: 0.4222\n",
      "  worst: 0.0000\n",
      "\n",
      "📝 Review 3:\n",
      "  time: 0.3878\n",
      "  waste: 0.3878\n",
      "  plot: 0.3878\n",
      "  complete: 0.3878\n",
      "  acting: 0.3878\n",
      "\n",
      "📝 Review 4:\n",
      "  surprisingly: 0.4472\n",
      "  good: 0.4472\n",
      "  compelling: 0.4472\n",
      "  characters: 0.4472\n",
      "  film: 0.4472\n",
      "\n",
      "📝 Review 5:\n",
      "  year: 0.3878\n",
      "  worst: 0.3878\n",
      "  seen: 0.3878\n",
      "  every: 0.3878\n",
      "  hated: 0.3878\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------\n",
    "# Task 3: Compare CountVectorizer and TdidfVectorizer \n",
    "#--------------------------------------------------------\n",
    "import numpy as np\n",
    "\n",
    "def top_tfidf_terms(vectorizer, tfidf_matrix, top_n=5):\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    for i in range(tfidf_matrix.shape[0]):\n",
    "        print(f\"\\n📝 Review {i+1}:\")\n",
    "        row = tfidf_matrix[i].toarray().flatten()\n",
    "        top_indices = row.argsort()[::-1][:top_n]\n",
    "        for idx in top_indices:\n",
    "            print(f\"  {feature_names[idx]}: {row[idx]:.4f}\")\n",
    "\n",
    "top_tfidf_terms(tfidf_vec, X_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7d2f43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "########### Exercise 3: Text Classification #############\n",
    "#########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8da6cad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Classification Report: Naive Bayes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         1\n",
      "   macro avg       1.00      1.00      1.00         1\n",
      "weighted avg       1.00      1.00      1.00         1\n",
      "\n",
      "\n",
      "📊 Classification Report: Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         1\n",
      "   macro avg       1.00      1.00      1.00         1\n",
      "weighted avg       1.00      1.00      1.00         1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------\n",
    "# Step 1: Implement and Compare Classifiers \n",
    "#--------------------------------------------------------\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Use TF-IDF vectors from NLTK-preprocessed text\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "X = tfidf_vec.fit_transform(df['text_clean'])\n",
    "y = df['sentiment']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define classifiers\n",
    "models = {\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000)\n",
    "}\n",
    "\n",
    "# Train and evaluate\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"\\n📊 Classification Report: {name}\")\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1638bf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results using: No Preprocessing\n",
      "\n",
      "Naive Bayes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       1.0\n",
      "           1       0.00      0.00      0.00       0.0\n",
      "\n",
      "    accuracy                           0.00       1.0\n",
      "   macro avg       0.00      0.00      0.00       1.0\n",
      "weighted avg       0.00      0.00      0.00       1.0\n",
      "\n",
      "\n",
      "Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       1.0\n",
      "           1       0.00      0.00      0.00       0.0\n",
      "\n",
      "    accuracy                           0.00       1.0\n",
      "   macro avg       0.00      0.00      0.00       1.0\n",
      "weighted avg       0.00      0.00      0.00       1.0\n",
      "\n",
      "Results using: NLTK Preprocessing\n",
      "\n",
      "Naive Bayes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         1\n",
      "   macro avg       1.00      1.00      1.00         1\n",
      "weighted avg       1.00      1.00      1.00         1\n",
      "\n",
      "\n",
      "Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         1\n",
      "   macro avg       1.00      1.00      1.00         1\n",
      "weighted avg       1.00      1.00      1.00         1\n",
      "\n",
      "Results using: spaCy Preprocessing\n",
      "\n",
      "Naive Bayes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         1\n",
      "   macro avg       1.00      1.00      1.00         1\n",
      "weighted avg       1.00      1.00      1.00         1\n",
      "\n",
      "\n",
      "Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         1\n",
      "   macro avg       1.00      1.00      1.00         1\n",
      "weighted avg       1.00      1.00      1.00         1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------\n",
    "# Step 2: Study Preprocessing Impact\n",
    "#--------------------------------------------------------\n",
    "\n",
    "# Define reusable evaluation function\n",
    "def evaluate_model(preprocessed_texts, labels, vectorizer=TfidfVectorizer(), title=\"\"):\n",
    "    X = vectorizer.fit_transform(preprocessed_texts)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    print(f\"Results using: {title}\")\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(f\"\\n{name}\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Compare no preprocessing vs NLTK vs spaCy:\n",
    "# 1. No preprocessing\n",
    "evaluate_model(df['review_text'], df['sentiment'], title=\"No Preprocessing\")\n",
    "\n",
    "# 2. NLTK preprocessed\n",
    "evaluate_model(df['text_clean'], df['sentiment'], title=\"NLTK Preprocessing\")\n",
    "\n",
    "# 3. spaCy preprocessed\n",
    "df['spacy_clean'] = df['spacy_tokens'].apply(lambda tokens: ' '.join(tokens))\n",
    "evaluate_model(df['spacy_clean'], df['sentiment'], title=\"spaCy Preprocessing\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a5dbe07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Top 5 Features (using chi2):\n",
      "['absolutely' 'fantastic' 'masterpiece' 'surprisingly' 'truly']\n",
      "\n",
      "🔍 Top 10 Features (using chi2):\n",
      "['absolutely' 'characters' 'compelling' 'fantastic' 'film' 'good'\n",
      " 'masterpiece' 'surprisingly' 'terrible' 'truly']\n",
      "\n",
      "🔍 Top 15 Features (using chi2):\n",
      "['absolutely' 'characters' 'compelling' 'fantastic' 'film' 'good' 'great'\n",
      " 'masterpiece' 'meh' 'okay' 'surprisingly' 'terrible' 'truly' 'worst'\n",
      " 'year']\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------\n",
    "# Step 3: Feature Selection Analysis\n",
    "#--------------------------------------------------------\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "def feature_selection_analysis(X, y, vectorizer, top_ks=[5, 10, 15]):\n",
    "    for k in top_ks:\n",
    "        print(f\"\\n🔍 Top {k} Features (using chi2):\")\n",
    "        selector = SelectKBest(score_func=chi2, k=k)\n",
    "        selector.fit(X, y)\n",
    "        mask = selector.get_support()\n",
    "        selected_features = vectorizer.get_feature_names_out()[mask]\n",
    "        print(selected_features)\n",
    "\n",
    "# Run Feature Selection\n",
    "# Use spaCy-preprocessed data here as an example\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_full = vectorizer.fit_transform(df['spacy_clean'])\n",
    "y_full = df['sentiment']\n",
    "\n",
    "feature_selection_analysis(X_full, y_full, vectorizer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

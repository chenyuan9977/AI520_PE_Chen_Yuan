{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "478493bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation complete\n"
     ]
    }
   ],
   "source": [
    "############################################################################\n",
    "##################### TASK 0: Data Preparation #############################\n",
    "############################################################################\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# a. Load the IMDB Movie Reviews dataset\n",
    "imdb_dataset = load_dataset('imdb')\n",
    "\n",
    "# b. Create a new dataframe with columns 'review_text' and 'sentiment'\n",
    "# The original columns are 'text' and 'label'\n",
    "train_df = pd.DataFrame({'review_text': imdb_dataset['train']['text'],\n",
    "                         'sentiment': imdb_dataset['train']['label']})\n",
    "\n",
    "# c. Map sentiment labels (0 -> 'negative', 1 -> 'positive')\n",
    "sentiment_mapping = {0: 'negative', 1: 'positive'}\n",
    "train_df['sentiment'] = train_df['sentiment'].map(sentiment_mapping)\n",
    "\n",
    "# d. Save the processed dataset\n",
    "train_df.to_csv('movie_reviews.csv', index=False)\n",
    "\n",
    "print(\"Data preparation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40ecb80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        rented curiousyellow video store controversy s...\n",
       "1        curious yellow risible pretentious steaming pi...\n",
       "2        avoid making type film future film interesting...\n",
       "3        film probably inspired godards masculin f√©mini...\n",
       "4        oh brotherafter hearing ridiculous film umptee...\n",
       "                               ...                        \n",
       "24995    hit time better categorised australian cult fi...\n",
       "24996    love movie like another time try explain virtu...\n",
       "24997    film sequel barry mckenzie holds two greatest ...\n",
       "24998    adventures barry mckenzie started life satiric...\n",
       "24999    story centers around barry mckenzie must go en...\n",
       "Name: review_text, Length: 25000, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############################################################################\n",
    "##################### TASK 1: Data Preprocessing ###########################\n",
    "############################################################################\n",
    "import re, string, nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Performs text preprocessing: lowercasing, punctuation removal, and stop word removal.\n",
    "    \"\"\"\n",
    "    # a. Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # b. Remove punctuation marks\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # c. Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    text = ' '.join(filtered_text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Load the dataframes if not already in memory from Task 0\n",
    "train_df = pd.read_csv('movie_reviews.csv')\n",
    "\n",
    "# Apply preprocessing to the 'review_text' column\n",
    "train_df['review_text'].apply(preprocess_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc533151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Task 2: Text tokenization complete. Added 'tokenized_sentences' and 'tokenized_words' columns.\n",
      "\n",
      "First 5 rows of train_df with tokenized data:\n",
      "                                         review_text  \\\n",
      "0  I rented I AM CURIOUS-YELLOW from my video sto...   \n",
      "1  \"I Am Curious: Yellow\" is a risible and preten...   \n",
      "2  If only to avoid making this type of film in t...   \n",
      "3  This film was probably inspired by Godard's Ma...   \n",
      "4  Oh, brother...after hearing about this ridicul...   \n",
      "\n",
      "                                 tokenized_sentences  \\\n",
      "0  [I rented I AM CURIOUS-YELLOW from my video st...   \n",
      "1  [\"I Am Curious: Yellow\" is a risible and prete...   \n",
      "2  [If only to avoid making this type of film in ...   \n",
      "3  [This film was probably inspired by Godard's M...   \n",
      "4  [Oh, brother...after hearing about this ridicu...   \n",
      "\n",
      "                                     tokenized_words  \n",
      "0  [I, rented, I, AM, CURIOUS-YELLOW, from, my, v...  \n",
      "1  [``, I, Am, Curious, :, Yellow, '', is, a, ris...  \n",
      "2  [If, only, to, avoid, making, this, type, of, ...  \n",
      "3  [This, film, was, probably, inspired, by, Goda...  \n",
      "4  [Oh, ,, brother, ..., after, hearing, about, t...  \n"
     ]
    }
   ],
   "source": [
    "############################################################################\n",
    "##################### TASK 2: Text Tokenization ############################\n",
    "############################################################################\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "import pandas as pd # Ensure pandas is imported\n",
    "\n",
    "# Ensure 'punkt' is downloaded (it should have been downloaded in Task 1,\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt.zip')\n",
    "except LookupError:\n",
    "    print(\"Downloading 'punkt' NLTK data...\")\n",
    "    nltk.download('punkt')\n",
    "    print(\"'punkt' downloaded.\")\n",
    "\n",
    "def tokenize_text_data(df_column):\n",
    "    \"\"\"\n",
    "    Performs sentence and word tokenization on a DataFrame column of text.\n",
    "    \"\"\"\n",
    "    # a. Perform sentence tokenization using NLTK\n",
    "    sentences = df_column.apply(sent_tokenize)\n",
    "    # b. Perform word tokenization using NLTK on the processed text\n",
    "    words = df_column.apply(word_tokenize)\n",
    "    return sentences, words\n",
    "\n",
    "# Load the dataframes\n",
    "train_df = pd.read_csv('movie_reviews.csv')\n",
    "\n",
    "# Apply tokenization to the 'review_text' column\n",
    "train_sentences, train_words = tokenize_text_data(train_df['review_text'])\n",
    "\n",
    "# c. Store tokenized sentences and words in new DataFrame columns\n",
    "train_df['tokenized_sentences'] = train_sentences\n",
    "train_df['tokenized_words'] = train_words\n",
    "\n",
    "print(\"\\nTask 2: Text tokenization complete. Added 'tokenized_sentences' and 'tokenized_words' columns.\")\n",
    "print(\"\\nFirst 5 rows of train_df with tokenized data:\")\n",
    "print(train_df[['review_text', 'tokenized_sentences', 'tokenized_words']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6462bf10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrames loaded for feature vectorization.\n",
      "\n",
      "Task 3: Basic Feature Vectorization complete.\n",
      "\n",
      "Shape of Bag-of-Words matrix for training data (documents x features): (25000, 1000)\n",
      "\n",
      "Top 20 most frequent words in the training data:\n",
      "- the: 336749\n",
      "- and: 164140\n",
      "- of: 145864\n",
      "- to: 135724\n",
      "- is: 107332\n",
      "- br: 101871\n",
      "- it: 96467\n",
      "- in: 93976\n",
      "- this: 76007\n",
      "- that: 73286\n",
      "- was: 48209\n",
      "- as: 46935\n",
      "- for: 44345\n",
      "- with: 44130\n",
      "- movie: 44047\n",
      "- but: 42623\n",
      "- film: 40159\n",
      "- you: 34267\n",
      "- on: 34202\n",
      "- not: 30632\n"
     ]
    }
   ],
   "source": [
    "############################################################################\n",
    "################## TASK 3: Basic Text Vectorization#########################\n",
    "############################################################################\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd # Ensure pandas is imported\n",
    "\n",
    "# Load the dataframes from Task 0\n",
    "print(\"DataFrames loaded for feature vectorization.\")\n",
    "train_df = pd.read_csv('movie_reviews.csv')\n",
    "\n",
    "# a. Create a Bag-of-Words representation using CountVectorizer\n",
    "# Limit to top 1000 features as per the prompt's implied requirement\n",
    "vectorizer = CountVectorizer(max_features=1000)\n",
    "\n",
    "# b. Apply it to the processed text column\n",
    "# Fit the vectorizer on the training data and transform both train and test\n",
    "# Fitting only on training data prevents data leakage from the test set.\n",
    "X_train = vectorizer.fit_transform(train_df['review_text'])\n",
    "\n",
    "# Get feature names (the words that form the columns of the BoW matrix)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# c. Calculate and analyze word frequencies\n",
    "# Sum the counts for each feature (word) across all documents in the training set\n",
    "word_frequencies = X_train.sum(axis=0)\n",
    "\n",
    "# Convert to a dictionary mapping feature names to their frequencies\n",
    "frequencies_dict = dict(zip(feature_names, word_frequencies.tolist()[0]))\n",
    "\n",
    "# Display the top 20 most frequent words\n",
    "# Sort the dictionary items by frequency in descending order\n",
    "sorted_frequencies = sorted(frequencies_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "print(\"\\nTask 3: Basic Feature Vectorization complete.\")\n",
    "print(\"\\nShape of Bag-of-Words matrix for training data (documents x features):\", X_train.shape)\n",
    "\n",
    "print(\"\\nTop 20 most frequent words in the training data:\")\n",
    "for word, freq in sorted_frequencies[:20]:\n",
    "    print(f\"- {word}: {freq}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
